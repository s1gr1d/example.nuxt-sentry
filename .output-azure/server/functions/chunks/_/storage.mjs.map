{"version":3,"file":"storage.mjs","sources":["../../../../../node_modules/@nuxt/content/dist/runtime/query/match/utils.js","../../../../../node_modules/@nuxt/content/dist/runtime/query/query.js","../../../../../node_modules/pathe/dist/shared/pathe.ff20891b.mjs","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/csv/create-tokenizer.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/csv/parser.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/csv/from-csv.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/csv/index.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/markdown.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/yaml.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/json.js","../../../../../node_modules/@nuxt/content/dist/runtime/transformers/index.js","../../../../../node_modules/@nuxt/content/dist/runtime/utils/config.js","../../../../../node_modules/@nuxt/content/dist/runtime/query/match/index.js","../../../../../node_modules/@nuxt/content/dist/runtime/query/match/pipeline.js","../../../../../node_modules/@nuxt/content/dist/runtime/server/content-index.js","../../../../../node_modules/@nuxt/content/dist/runtime/server/storage.js"],"sourcesContent":["export const get = (obj, path) => path.split(\".\").reduce((acc, part) => acc && acc[part], obj);\nconst _pick = (obj, condition) => Object.keys(obj).filter(condition).reduce((newObj, key) => Object.assign(newObj, { [key]: obj[key] }), {});\nexport const pick = (keys) => (obj) => keys && keys.length ? _pick(obj, (key) => keys.includes(key)) : obj;\nexport const omit = (keys) => (obj) => keys && keys.length ? _pick(obj, (key) => !keys.includes(key)) : obj;\nexport const apply = (fn) => (data) => Array.isArray(data) ? data.map((item) => fn(item)) : fn(data);\nexport const detectProperties = (keys) => {\n  const prefixes = [];\n  const properties = [];\n  for (const key of keys) {\n    if ([\"$\", \"_\"].includes(key)) {\n      prefixes.push(key);\n    } else {\n      properties.push(key);\n    }\n  }\n  return { prefixes, properties };\n};\nexport const withoutKeys = (keys = []) => (obj) => {\n  if (keys.length === 0 || !obj) {\n    return obj;\n  }\n  const { prefixes, properties } = detectProperties(keys);\n  return _pick(obj, (key) => !properties.includes(key) && !prefixes.includes(key[0]));\n};\nexport const withKeys = (keys = []) => (obj) => {\n  if (keys.length === 0 || !obj) {\n    return obj;\n  }\n  const { prefixes, properties } = detectProperties(keys);\n  return _pick(obj, (key) => properties.includes(key) || prefixes.includes(key[0]));\n};\nexport const sortList = (data, params) => {\n  const comperable = new Intl.Collator(params.$locale, {\n    numeric: params.$numeric,\n    caseFirst: params.$caseFirst,\n    sensitivity: params.$sensitivity\n  });\n  const keys = Object.keys(params).filter((key) => !key.startsWith(\"$\"));\n  for (const key of keys) {\n    data = data.sort((a, b) => {\n      const values = [get(a, key), get(b, key)].map((value) => {\n        if (value === null) {\n          return void 0;\n        }\n        if (value instanceof Date) {\n          return value.toISOString();\n        }\n        return value;\n      });\n      if (params[key] === -1) {\n        values.reverse();\n      }\n      return comperable.compare(values[0], values[1]);\n    });\n  }\n  return data;\n};\nexport const assertArray = (value, message = \"Expected an array\") => {\n  if (!Array.isArray(value)) {\n    throw new TypeError(message);\n  }\n};\nexport const ensureArray = (value) => {\n  return Array.isArray(value) ? value : [void 0, null].includes(value) ? [] : [value];\n};\n","import { ensureArray } from \"./match/utils.js\";\nconst arrayParams = [\"sort\", \"where\", \"only\", \"without\"];\nexport function createQuery(fetcher, opts = {}) {\n  const queryParams = {};\n  for (const key of Object.keys(opts.initialParams || {})) {\n    queryParams[key] = arrayParams.includes(key) ? ensureArray(opts.initialParams[key]) : opts.initialParams[key];\n  }\n  const $set = (key, fn = (v) => v) => {\n    return (...values) => {\n      queryParams[key] = fn(...values);\n      return query;\n    };\n  };\n  const resolveResult = (result) => {\n    if (opts.legacy) {\n      if (result?.surround) {\n        return result.surround;\n      }\n      if (!result) {\n        return result;\n      }\n      if (result?.dirConfig) {\n        result.result = {\n          _path: result.dirConfig?._path,\n          ...result.result,\n          _dir: result.dirConfig\n        };\n      }\n      return result?._path || Array.isArray(result) || !Object.prototype.hasOwnProperty.call(result, \"result\") ? result : result?.result;\n    }\n    return result;\n  };\n  const query = {\n    params: () => ({\n      ...queryParams,\n      ...queryParams.where ? { where: [...ensureArray(queryParams.where)] } : {},\n      ...queryParams.sort ? { sort: [...ensureArray(queryParams.sort)] } : {}\n    }),\n    only: $set(\"only\", ensureArray),\n    without: $set(\"without\", ensureArray),\n    where: $set(\"where\", (q) => [...ensureArray(queryParams.where), ...ensureArray(q)]),\n    sort: $set(\"sort\", (sort) => [...ensureArray(queryParams.sort), ...ensureArray(sort)]),\n    limit: $set(\"limit\", (v) => parseInt(String(v), 10)),\n    skip: $set(\"skip\", (v) => parseInt(String(v), 10)),\n    // find\n    find: () => fetcher(query).then(resolveResult),\n    findOne: () => fetcher($set(\"first\")(true)).then(resolveResult),\n    count: () => fetcher($set(\"count\")(true)).then(resolveResult),\n    // locale\n    locale: (_locale) => query.where({ _locale }),\n    withSurround: $set(\"surround\", (surroundQuery, options) => ({ query: surroundQuery, ...options })),\n    withDirConfig: () => $set(\"dirConfig\")(true)\n  };\n  if (opts.legacy) {\n    query.findSurround = (surroundQuery, options) => {\n      return query.withSurround(surroundQuery, options).find().then(resolveResult);\n    };\n    return query;\n  }\n  return query;\n}\n","const _DRIVE_LETTER_START_RE = /^[A-Za-z]:\\//;\nfunction normalizeWindowsPath(input = \"\") {\n  if (!input) {\n    return input;\n  }\n  return input.replace(/\\\\/g, \"/\").replace(_DRIVE_LETTER_START_RE, (r) => r.toUpperCase());\n}\n\nconst _UNC_REGEX = /^[/\\\\]{2}/;\nconst _IS_ABSOLUTE_RE = /^[/\\\\](?![/\\\\])|^[/\\\\]{2}(?!\\.)|^[A-Za-z]:[/\\\\]/;\nconst _DRIVE_LETTER_RE = /^[A-Za-z]:$/;\nconst _ROOT_FOLDER_RE = /^\\/([A-Za-z]:)?$/;\nconst sep = \"/\";\nconst delimiter = \":\";\nconst normalize = function(path) {\n  if (path.length === 0) {\n    return \".\";\n  }\n  path = normalizeWindowsPath(path);\n  const isUNCPath = path.match(_UNC_REGEX);\n  const isPathAbsolute = isAbsolute(path);\n  const trailingSeparator = path[path.length - 1] === \"/\";\n  path = normalizeString(path, !isPathAbsolute);\n  if (path.length === 0) {\n    if (isPathAbsolute) {\n      return \"/\";\n    }\n    return trailingSeparator ? \"./\" : \".\";\n  }\n  if (trailingSeparator) {\n    path += \"/\";\n  }\n  if (_DRIVE_LETTER_RE.test(path)) {\n    path += \"/\";\n  }\n  if (isUNCPath) {\n    if (!isPathAbsolute) {\n      return `//./${path}`;\n    }\n    return `//${path}`;\n  }\n  return isPathAbsolute && !isAbsolute(path) ? `/${path}` : path;\n};\nconst join = function(...arguments_) {\n  if (arguments_.length === 0) {\n    return \".\";\n  }\n  let joined;\n  for (const argument of arguments_) {\n    if (argument && argument.length > 0) {\n      if (joined === void 0) {\n        joined = argument;\n      } else {\n        joined += `/${argument}`;\n      }\n    }\n  }\n  if (joined === void 0) {\n    return \".\";\n  }\n  return normalize(joined.replace(/\\/\\/+/g, \"/\"));\n};\nfunction cwd() {\n  if (typeof process !== \"undefined\" && typeof process.cwd === \"function\") {\n    return process.cwd().replace(/\\\\/g, \"/\");\n  }\n  return \"/\";\n}\nconst resolve = function(...arguments_) {\n  arguments_ = arguments_.map((argument) => normalizeWindowsPath(argument));\n  let resolvedPath = \"\";\n  let resolvedAbsolute = false;\n  for (let index = arguments_.length - 1; index >= -1 && !resolvedAbsolute; index--) {\n    const path = index >= 0 ? arguments_[index] : cwd();\n    if (!path || path.length === 0) {\n      continue;\n    }\n    resolvedPath = `${path}/${resolvedPath}`;\n    resolvedAbsolute = isAbsolute(path);\n  }\n  resolvedPath = normalizeString(resolvedPath, !resolvedAbsolute);\n  if (resolvedAbsolute && !isAbsolute(resolvedPath)) {\n    return `/${resolvedPath}`;\n  }\n  return resolvedPath.length > 0 ? resolvedPath : \".\";\n};\nfunction normalizeString(path, allowAboveRoot) {\n  let res = \"\";\n  let lastSegmentLength = 0;\n  let lastSlash = -1;\n  let dots = 0;\n  let char = null;\n  for (let index = 0; index <= path.length; ++index) {\n    if (index < path.length) {\n      char = path[index];\n    } else if (char === \"/\") {\n      break;\n    } else {\n      char = \"/\";\n    }\n    if (char === \"/\") {\n      if (lastSlash === index - 1 || dots === 1) ; else if (dots === 2) {\n        if (res.length < 2 || lastSegmentLength !== 2 || res[res.length - 1] !== \".\" || res[res.length - 2] !== \".\") {\n          if (res.length > 2) {\n            const lastSlashIndex = res.lastIndexOf(\"/\");\n            if (lastSlashIndex === -1) {\n              res = \"\";\n              lastSegmentLength = 0;\n            } else {\n              res = res.slice(0, lastSlashIndex);\n              lastSegmentLength = res.length - 1 - res.lastIndexOf(\"/\");\n            }\n            lastSlash = index;\n            dots = 0;\n            continue;\n          } else if (res.length > 0) {\n            res = \"\";\n            lastSegmentLength = 0;\n            lastSlash = index;\n            dots = 0;\n            continue;\n          }\n        }\n        if (allowAboveRoot) {\n          res += res.length > 0 ? \"/..\" : \"..\";\n          lastSegmentLength = 2;\n        }\n      } else {\n        if (res.length > 0) {\n          res += `/${path.slice(lastSlash + 1, index)}`;\n        } else {\n          res = path.slice(lastSlash + 1, index);\n        }\n        lastSegmentLength = index - lastSlash - 1;\n      }\n      lastSlash = index;\n      dots = 0;\n    } else if (char === \".\" && dots !== -1) {\n      ++dots;\n    } else {\n      dots = -1;\n    }\n  }\n  return res;\n}\nconst isAbsolute = function(p) {\n  return _IS_ABSOLUTE_RE.test(p);\n};\nconst toNamespacedPath = function(p) {\n  return normalizeWindowsPath(p);\n};\nconst _EXTNAME_RE = /.(\\.[^./]+)$/;\nconst extname = function(p) {\n  const match = _EXTNAME_RE.exec(normalizeWindowsPath(p));\n  return match && match[1] || \"\";\n};\nconst relative = function(from, to) {\n  const _from = resolve(from).replace(_ROOT_FOLDER_RE, \"$1\").split(\"/\");\n  const _to = resolve(to).replace(_ROOT_FOLDER_RE, \"$1\").split(\"/\");\n  if (_to[0][1] === \":\" && _from[0][1] === \":\" && _from[0] !== _to[0]) {\n    return _to.join(\"/\");\n  }\n  const _fromCopy = [..._from];\n  for (const segment of _fromCopy) {\n    if (_to[0] !== segment) {\n      break;\n    }\n    _from.shift();\n    _to.shift();\n  }\n  return [..._from.map(() => \"..\"), ..._to].join(\"/\");\n};\nconst dirname = function(p) {\n  const segments = normalizeWindowsPath(p).replace(/\\/$/, \"\").split(\"/\").slice(0, -1);\n  if (segments.length === 1 && _DRIVE_LETTER_RE.test(segments[0])) {\n    segments[0] += \"/\";\n  }\n  return segments.join(\"/\") || (isAbsolute(p) ? \"/\" : \".\");\n};\nconst format = function(p) {\n  const segments = [p.root, p.dir, p.base ?? p.name + p.ext].filter(Boolean);\n  return normalizeWindowsPath(\n    p.root ? resolve(...segments) : segments.join(\"/\")\n  );\n};\nconst basename = function(p, extension) {\n  const lastSegment = normalizeWindowsPath(p).split(\"/\").pop();\n  return extension && lastSegment.endsWith(extension) ? lastSegment.slice(0, -extension.length) : lastSegment;\n};\nconst parse = function(p) {\n  const root = normalizeWindowsPath(p).split(\"/\").shift() || \"/\";\n  const base = basename(p);\n  const extension = extname(base);\n  return {\n    root,\n    dir: dirname(p),\n    base,\n    ext: extension,\n    name: base.slice(0, base.length - extension.length)\n  };\n};\n\nconst path = {\n  __proto__: null,\n  basename: basename,\n  delimiter: delimiter,\n  dirname: dirname,\n  extname: extname,\n  format: format,\n  isAbsolute: isAbsolute,\n  join: join,\n  normalize: normalize,\n  normalizeString: normalizeString,\n  parse: parse,\n  relative: relative,\n  resolve: resolve,\n  sep: sep,\n  toNamespacedPath: toNamespacedPath\n};\n\nexport { normalize as a, normalizeString as b, relative as c, delimiter as d, extname as e, dirname as f, format as g, basename as h, isAbsolute as i, join as j, parse as k, normalizeWindowsPath as n, path as p, resolve as r, sep as s, toNamespacedPath as t };\n","import { markdownLineEnding } from \"micromark-util-character\";\nimport { push, splice } from \"micromark-util-chunked\";\nimport { resolveAll } from \"micromark-util-resolve-all\";\nexport function createTokenizer(parser, initialize, from) {\n  let point = Object.assign(\n    from ? Object.assign({}, from) : {\n      line: 1,\n      column: 1,\n      offset: 0\n    },\n    {\n      _index: 0,\n      _bufferIndex: -1\n    }\n  );\n  const columnStart = {};\n  const resolveAllConstructs = [];\n  let chunks = [];\n  let stack = [];\n  let consumed = true;\n  const effects = {\n    consume,\n    enter,\n    exit,\n    attempt: constructFactory(onsuccessfulconstruct),\n    check: constructFactory(onsuccessfulcheck),\n    interrupt: constructFactory(onsuccessfulcheck, {\n      interrupt: true\n    })\n  };\n  const context = {\n    previous: null,\n    code: null,\n    containerState: {},\n    events: [],\n    parser,\n    sliceStream,\n    sliceSerialize,\n    now,\n    defineSkip,\n    write\n  };\n  let state = initialize.tokenize.call(context, effects);\n  let expectedCode;\n  if (initialize.resolveAll) {\n    resolveAllConstructs.push(initialize);\n  }\n  return context;\n  function write(slice) {\n    chunks = push(chunks, slice);\n    main();\n    if (chunks[chunks.length - 1] !== null) {\n      return [];\n    }\n    addResult(initialize, 0);\n    context.events = resolveAll(resolveAllConstructs, context.events, context);\n    return context.events;\n  }\n  function sliceSerialize(token, expandTabs) {\n    return serializeChunks(sliceStream(token), expandTabs);\n  }\n  function sliceStream(token) {\n    return sliceChunks(chunks, token);\n  }\n  function now() {\n    return Object.assign({}, point);\n  }\n  function defineSkip(value) {\n    columnStart[value.line] = value.column;\n    accountForPotentialSkip();\n  }\n  function main() {\n    let chunkIndex;\n    while (point._index < chunks.length) {\n      const chunk = chunks[point._index];\n      if (typeof chunk === \"string\") {\n        chunkIndex = point._index;\n        if (point._bufferIndex < 0) {\n          point._bufferIndex = 0;\n        }\n        while (point._index === chunkIndex && point._bufferIndex < chunk.length) {\n          go(chunk.charCodeAt(point._bufferIndex));\n        }\n      } else {\n        go(chunk);\n      }\n    }\n  }\n  function go(code) {\n    consumed = void 0;\n    expectedCode = code;\n    state = state(code);\n  }\n  function consume(code) {\n    if (markdownLineEnding(code)) {\n      point.line++;\n      point.column = 1;\n      point.offset += code === -3 ? 2 : 1;\n      accountForPotentialSkip();\n    } else if (code !== -1) {\n      point.column++;\n      point.offset++;\n    }\n    if (point._bufferIndex < 0) {\n      point._index++;\n    } else {\n      point._bufferIndex++;\n      if (point._bufferIndex === chunks[point._index].length) {\n        point._bufferIndex = -1;\n        point._index++;\n      }\n    }\n    context.previous = code;\n    consumed = true;\n  }\n  function enter(type, fields) {\n    const token = fields || {};\n    token.type = type;\n    token.start = now();\n    context.events.push([\"enter\", token, context]);\n    stack.push(token);\n    return token;\n  }\n  function exit(type) {\n    const token = stack.pop();\n    token.end = now();\n    context.events.push([\"exit\", token, context]);\n    return token;\n  }\n  function onsuccessfulconstruct(construct, info) {\n    addResult(construct, info.from);\n  }\n  function onsuccessfulcheck(_, info) {\n    info.restore();\n  }\n  function constructFactory(onreturn, fields) {\n    return hook;\n    function hook(constructs, returnState, bogusState) {\n      let listOfConstructs;\n      let constructIndex;\n      let currentConstruct;\n      let info;\n      return Array.isArray(constructs) ? (\n        /* c8 ignore next 1 */\n        handleListOfConstructs(constructs)\n      ) : \"tokenize\" in constructs ? handleListOfConstructs([constructs]) : handleMapOfConstructs(constructs);\n      function handleMapOfConstructs(map) {\n        return start;\n        function start(code) {\n          const def = code !== null && map[code];\n          const all = code !== null && map.null;\n          const list = [\n            // To do: add more extension tests.\n            /* c8 ignore next 2 */\n            ...Array.isArray(def) ? def : def ? [def] : [],\n            ...Array.isArray(all) ? all : all ? [all] : []\n          ];\n          return handleListOfConstructs(list)(code);\n        }\n      }\n      function handleListOfConstructs(list) {\n        listOfConstructs = list;\n        constructIndex = 0;\n        if (list.length === 0) {\n          return bogusState;\n        }\n        return handleConstruct(list[constructIndex]);\n      }\n      function handleConstruct(construct) {\n        return start;\n        function start(code) {\n          info = store();\n          currentConstruct = construct;\n          if (!construct.partial) {\n            context.currentConstruct = construct;\n          }\n          if (construct.name && context.parser.constructs.disable.null.includes(construct.name)) {\n            return nok(code);\n          }\n          return construct.tokenize.call(\n            // If we do have fields, create an object w/ `context` as its\n            // prototype.\n            // This allows a “live binding”, which is needed for `interrupt`.\n            fields ? Object.assign(Object.create(context), fields) : context,\n            effects,\n            ok,\n            nok\n          )(code);\n        }\n      }\n      function ok(code) {\n        consumed = true;\n        onreturn(currentConstruct, info);\n        return returnState;\n      }\n      function nok(code) {\n        consumed = true;\n        info.restore();\n        if (++constructIndex < listOfConstructs.length) {\n          return handleConstruct(listOfConstructs[constructIndex]);\n        }\n        return bogusState;\n      }\n    }\n  }\n  function addResult(construct, from2) {\n    if (construct.resolveAll && !resolveAllConstructs.includes(construct)) {\n      resolveAllConstructs.push(construct);\n    }\n    if (construct.resolve) {\n      splice(\n        context.events,\n        from2,\n        context.events.length - from2,\n        construct.resolve(context.events.slice(from2), context)\n      );\n    }\n    if (construct.resolveTo) {\n      context.events = construct.resolveTo(context.events, context);\n    }\n  }\n  function store() {\n    const startPoint = now();\n    const startPrevious = context.previous;\n    const startCurrentConstruct = context.currentConstruct;\n    const startEventsIndex = context.events.length;\n    const startStack = Array.from(stack);\n    return {\n      restore,\n      from: startEventsIndex\n    };\n    function restore() {\n      point = startPoint;\n      context.previous = startPrevious;\n      context.currentConstruct = startCurrentConstruct;\n      context.events.length = startEventsIndex;\n      stack = startStack;\n      accountForPotentialSkip();\n    }\n  }\n  function accountForPotentialSkip() {\n    if (point.line in columnStart && point.column < 2) {\n      point.column = columnStart[point.line];\n      point.offset += columnStart[point.line] - 1;\n    }\n  }\n}\nfunction sliceChunks(chunks, token) {\n  const startIndex = token.start._index;\n  const startBufferIndex = token.start._bufferIndex;\n  const endIndex = token.end._index;\n  const endBufferIndex = token.end._bufferIndex;\n  let view;\n  if (startIndex === endIndex) {\n    view = [chunks[startIndex].slice(startBufferIndex, endBufferIndex)];\n  } else {\n    view = chunks.slice(startIndex, endIndex);\n    if (startBufferIndex > -1) {\n      view[0] = view[0].slice(startBufferIndex);\n    }\n    if (endBufferIndex > 0) {\n      view.push(chunks[endIndex].slice(0, endBufferIndex));\n    }\n  }\n  return view;\n}\nfunction serializeChunks(chunks, expandTabs) {\n  let index = -1;\n  const result = [];\n  let atTab;\n  while (++index < chunks.length) {\n    const chunk = chunks[index];\n    let value;\n    if (typeof chunk === \"string\") {\n      value = chunk;\n    } else\n      switch (chunk) {\n        case -5: {\n          value = \"\\r\";\n          break;\n        }\n        case -4: {\n          value = \"\\n\";\n          break;\n        }\n        case -3: {\n          value = \"\\r\\n\";\n          break;\n        }\n        case -2: {\n          value = expandTabs ? \" \" : \"\t\";\n          break;\n        }\n        case -1: {\n          if (!expandTabs && atTab) continue;\n          value = \" \";\n          break;\n        }\n        default: {\n          value = String.fromCharCode(chunk);\n        }\n      }\n    atTab = chunk === -2;\n    result.push(value);\n  }\n  return result.join(\"\");\n}\n","import { markdownLineEnding, markdownSpace } from \"micromark-util-character\";\nimport { createTokenizer } from \"./create-tokenizer.js\";\nfunction initializeDocument(effects) {\n  const self = this;\n  const delimiter = (this.parser.delimiter || \",\").charCodeAt(0);\n  return enterRow;\n  function enterRow(code) {\n    return effects.attempt(\n      { tokenize: attemptLastLine },\n      (code2) => {\n        effects.consume(code2);\n        return enterRow;\n      },\n      (code2) => {\n        effects.enter(\"row\");\n        return enterColumn(code2);\n      }\n    )(code);\n  }\n  function enterColumn(code) {\n    effects.enter(\"column\");\n    return content(code);\n  }\n  function content(code) {\n    if (code === null) {\n      effects.exit(\"column\");\n      effects.exit(\"row\");\n      effects.consume(code);\n      return content;\n    }\n    if (code === 34) {\n      return quotedData(code);\n    }\n    if (code === delimiter) {\n      if (self.previous === delimiter || markdownLineEnding(self.previous) || self.previous === null) {\n        effects.enter(\"data\");\n        effects.exit(\"data\");\n      }\n      effects.exit(\"column\");\n      effects.enter(\"columnSeparator\");\n      effects.consume(code);\n      effects.exit(\"columnSeparator\");\n      effects.enter(\"column\");\n      return content;\n    }\n    if (markdownLineEnding(code)) {\n      effects.exit(\"column\");\n      effects.enter(\"newline\");\n      effects.consume(code);\n      effects.exit(\"newline\");\n      effects.exit(\"row\");\n      return enterRow;\n    }\n    return data(code);\n  }\n  function data(code) {\n    effects.enter(\"data\");\n    return dataChunk(code);\n  }\n  function dataChunk(code) {\n    if (code === null || markdownLineEnding(code) || code === delimiter) {\n      effects.exit(\"data\");\n      return content(code);\n    }\n    if (code === 92) {\n      return escapeCharacter(code);\n    }\n    effects.consume(code);\n    return dataChunk;\n  }\n  function escapeCharacter(code) {\n    effects.consume(code);\n    return function(code2) {\n      effects.consume(code2);\n      return content;\n    };\n  }\n  function quotedData(code) {\n    effects.enter(\"quotedData\");\n    effects.enter(\"quotedDataChunk\");\n    effects.consume(code);\n    return quotedDataChunk;\n  }\n  function quotedDataChunk(code) {\n    if (code === 92) {\n      return escapeCharacter(code);\n    }\n    if (code === 34) {\n      return effects.attempt(\n        { tokenize: attemptDoubleQuote },\n        (code2) => {\n          effects.exit(\"quotedDataChunk\");\n          effects.enter(\"quotedDataChunk\");\n          return quotedDataChunk(code2);\n        },\n        (code2) => {\n          effects.consume(code2);\n          effects.exit(\"quotedDataChunk\");\n          effects.exit(\"quotedData\");\n          return content;\n        }\n      )(code);\n    }\n    effects.consume(code);\n    return quotedDataChunk;\n  }\n}\nfunction attemptDoubleQuote(effects, ok, nok) {\n  return startSequence;\n  function startSequence(code) {\n    if (code !== 34) {\n      return nok(code);\n    }\n    effects.enter(\"quoteFence\");\n    effects.consume(code);\n    return sequence;\n  }\n  function sequence(code) {\n    if (code !== 34) {\n      return nok(code);\n    }\n    effects.consume(code);\n    effects.exit(\"quoteFence\");\n    return (code2) => ok(code2);\n  }\n}\nfunction attemptLastLine(effects, ok, nok) {\n  return enterLine;\n  function enterLine(code) {\n    if (!markdownSpace(code) && code !== null) {\n      return nok(code);\n    }\n    effects.enter(\"emptyLine\");\n    return continueLine(code);\n  }\n  function continueLine(code) {\n    if (markdownSpace(code)) {\n      effects.consume(code);\n      return continueLine;\n    }\n    if (code === null) {\n      effects.exit(\"emptyLine\");\n      return ok(code);\n    }\n    return nok(code);\n  }\n}\nexport const parse = (options) => {\n  return createTokenizer(\n    { ...options },\n    { tokenize: initializeDocument },\n    void 0\n  );\n};\n","import { toString } from \"mdast-util-to-string\";\nimport { preprocess, postprocess } from \"micromark\";\nimport { stringifyPosition } from \"unist-util-stringify-position\";\nimport { parse } from \"./parser.js\";\nconst own = {}.hasOwnProperty;\nconst initialPoint = {\n  line: 1,\n  column: 1,\n  offset: 0\n};\nexport const fromCSV = function(value, encoding, options) {\n  if (typeof encoding !== \"string\") {\n    options = encoding;\n    encoding = void 0;\n  }\n  return compiler()(\n    postprocess(\n      parse(options).write(preprocess()(value, encoding, true))\n    )\n  );\n};\nfunction compiler() {\n  const config = {\n    enter: {\n      column: opener(openColumn),\n      row: opener(openRow),\n      data: onenterdata,\n      quotedData: onenterdata\n    },\n    exit: {\n      row: closer(),\n      column: closer(),\n      data: onexitdata,\n      quotedData: onexitQuotedData\n    }\n  };\n  return compile;\n  function compile(events) {\n    const tree = {\n      type: \"root\",\n      children: []\n    };\n    const stack = [tree];\n    const tokenStack = [];\n    const context = {\n      stack,\n      tokenStack,\n      config,\n      enter,\n      exit,\n      resume\n    };\n    let index = -1;\n    while (++index < events.length) {\n      const handler = config[events[index][0]];\n      if (own.call(handler, events[index][1].type)) {\n        handler[events[index][1].type].call(\n          Object.assign(\n            {\n              sliceSerialize: events[index][2].sliceSerialize\n            },\n            context\n          ),\n          events[index][1]\n        );\n      }\n    }\n    if (tokenStack.length > 0) {\n      const tail = tokenStack[tokenStack.length - 1];\n      const handler = tail[1] || defaultOnError;\n      handler.call(context, void 0, tail[0]);\n    }\n    tree.position = {\n      start: point(\n        events.length > 0 ? events[0][1].start : initialPoint\n      ),\n      end: point(\n        events.length > 0 ? events[events.length - 2][1].end : initialPoint\n      )\n    };\n    return tree;\n  }\n  function point(d) {\n    return {\n      line: d.line,\n      column: d.column,\n      offset: d.offset\n    };\n  }\n  function opener(create, and) {\n    return open;\n    function open(token) {\n      enter.call(this, create(token), token);\n      if (and) {\n        and.call(this, token);\n      }\n    }\n  }\n  function enter(node, token, errorHandler) {\n    const parent = this.stack[this.stack.length - 1];\n    parent.children.push(node);\n    this.stack.push(node);\n    this.tokenStack.push([token, errorHandler]);\n    node.position = {\n      start: point(token.start)\n    };\n    return node;\n  }\n  function closer(and) {\n    return close;\n    function close(token) {\n      if (and) {\n        and.call(this, token);\n      }\n      exit.call(this, token);\n    }\n  }\n  function exit(token, onExitError) {\n    const node = this.stack.pop();\n    const open = this.tokenStack.pop();\n    if (!open) {\n      throw new Error(\n        \"Cannot close `\" + token.type + \"` (\" + stringifyPosition({\n          start: token.start,\n          end: token.end\n        }) + \"): it\\u2019s not open\"\n      );\n    } else if (open[0].type !== token.type) {\n      if (onExitError) {\n        onExitError.call(this, token, open[0]);\n      } else {\n        const handler = open[1] || defaultOnError;\n        handler.call(this, token, open[0]);\n      }\n    }\n    node.position.end = point(token.end);\n    return node;\n  }\n  function resume() {\n    return toString(this.stack.pop());\n  }\n  function onenterdata(token) {\n    const parent = this.stack[this.stack.length - 1];\n    let tail = parent.children[parent.children.length - 1];\n    if (!tail || tail.type !== \"text\") {\n      tail = text();\n      tail.position = {\n        start: point(token.start)\n      };\n      parent.children.push(tail);\n    }\n    this.stack.push(tail);\n  }\n  function onexitdata(token) {\n    const tail = this.stack.pop();\n    tail.value += this.sliceSerialize(token).trim().replace(/\"\"/g, '\"');\n    tail.position.end = point(token.end);\n  }\n  function onexitQuotedData(token) {\n    const tail = this.stack.pop();\n    const value = this.sliceSerialize(token);\n    tail.value += this.sliceSerialize(token).trim().substring(1, value.length - 1).replace(/\"\"/g, '\"');\n    tail.position.end = point(token.end);\n  }\n  function text() {\n    return {\n      type: \"text\",\n      value: \"\"\n    };\n  }\n  function openColumn() {\n    return {\n      type: \"column\",\n      children: []\n    };\n  }\n  function openRow() {\n    return {\n      type: \"row\",\n      children: []\n    };\n  }\n}\nfunction defaultOnError(left, right) {\n  if (left) {\n    throw new Error(\n      \"Cannot close `\" + left.type + \"` (\" + stringifyPosition({\n        start: left.start,\n        end: left.end\n      }) + \"): a different token (`\" + right.type + \"`, \" + stringifyPosition({\n        start: right.start,\n        end: right.end\n      }) + \") is open\"\n    );\n  } else {\n    throw new Error(\n      \"Cannot close document, a token (`\" + right.type + \"`, \" + stringifyPosition({\n        start: right.start,\n        end: right.end\n      }) + \") is still open\"\n    );\n  }\n}\n","import { unified } from \"unified\";\nimport { defineTransformer } from \"../utils.js\";\nimport { fromCSV } from \"./from-csv.js\";\nfunction csvParse(options) {\n  const parser = (doc) => {\n    return fromCSV(doc, options);\n  };\n  Object.assign(this, { Parser: parser });\n  const toJsonObject = (tree) => {\n    const [header, ...rows] = tree.children;\n    const columns = header.children.map((col) => col.children[0].value);\n    const data = rows.map((row) => {\n      return row.children.reduce((acc, col, i) => {\n        acc[String(columns[i])] = col.children[0]?.value;\n        return acc;\n      }, {});\n    });\n    return data;\n  };\n  const toJsonArray = (tree) => {\n    const data = tree.children.map((row) => {\n      return row.children.map((col) => col.children[0]?.value);\n    });\n    return data;\n  };\n  const compiler = (doc) => {\n    if (options.json) {\n      return toJsonObject(doc);\n    }\n    return toJsonArray(doc);\n  };\n  Object.assign(this, { Compiler: compiler });\n}\nexport default defineTransformer({\n  name: \"csv\",\n  extensions: [\".csv\"],\n  parse: async (_id, content, options = {}) => {\n    const stream = unified().use(csvParse, {\n      delimiter: \",\",\n      json: true,\n      ...options\n    });\n    const { result } = await stream.process(content);\n    return {\n      _id,\n      _type: \"csv\",\n      body: result\n    };\n  }\n});\n","import { parseMarkdown } from \"@nuxtjs/mdc/runtime\";\nimport { normalizeUri } from \"micromark-util-sanitize-uri\";\nimport { isRelative } from \"ufo\";\nimport { defineTransformer } from \"./utils.js\";\nimport { generatePath } from \"./path-meta.js\";\nexport default defineTransformer({\n  name: \"markdown\",\n  extensions: [\".md\"],\n  parse: async (_id, content, options = {}) => {\n    const config = { ...options };\n    config.rehypePlugins = await importPlugins(config.rehypePlugins);\n    config.remarkPlugins = await importPlugins(config.remarkPlugins);\n    const highlightOptions = options.highlight ? {\n      ...options.highlight,\n      // Pass only when it's an function. String values are handled by `@nuxtjs/mdc`\n      highlighter: typeof options.highlight?.highlighter === \"function\" ? options.highlight.highlighter : void 0\n    } : void 0;\n    const parsed = await parseMarkdown(content, {\n      ...config,\n      highlight: highlightOptions,\n      remark: {\n        plugins: config.remarkPlugins\n      },\n      rehype: {\n        options: {\n          handlers: {\n            link\n          }\n        },\n        plugins: config.rehypePlugins\n      },\n      toc: config.toc\n    });\n    return {\n      ...parsed.data,\n      excerpt: parsed.excerpt,\n      body: {\n        ...parsed.body,\n        toc: parsed.toc\n      },\n      _type: \"markdown\",\n      _id\n    };\n  }\n});\nasync function importPlugins(plugins = {}) {\n  const resolvedPlugins = {};\n  for (const [name, plugin] of Object.entries(plugins)) {\n    if (plugin) {\n      resolvedPlugins[name] = {\n        instance: plugin.instance || await import(\n          /* @vite-ignore */\n          name\n        ).then((m) => m.default || m),\n        options: plugin\n      };\n    } else {\n      resolvedPlugins[name] = false;\n    }\n  }\n  return resolvedPlugins;\n}\nfunction link(state, node) {\n  const properties = {\n    ...node.attributes || {},\n    href: normalizeUri(normalizeLink(node.url))\n  };\n  if (node.title !== null && node.title !== void 0) {\n    properties.title = node.title;\n  }\n  const result = {\n    type: \"element\",\n    tagName: \"a\",\n    properties,\n    children: state.all(node)\n  };\n  state.patch(node, result);\n  return state.applyData(node, result);\n}\nfunction normalizeLink(link2) {\n  const match = link2.match(/#.+$/);\n  const hash = match ? match[0] : \"\";\n  if (link2.replace(/#.+$/, \"\").endsWith(\".md\") && (isRelative(link2) || !/^https?/.test(link2) && !link2.startsWith(\"/\"))) {\n    return generatePath(link2.replace(\".md\" + hash, \"\"), { forceLeadingSlash: false }) + hash;\n  } else {\n    return link2;\n  }\n}\n","import { parseFrontMatter } from \"remark-mdc\";\nimport { defineTransformer } from \"./utils.js\";\nexport default defineTransformer({\n  name: \"Yaml\",\n  extensions: [\".yml\", \".yaml\"],\n  parse: (_id, content) => {\n    const { data } = parseFrontMatter(`---\n${content}\n---`);\n    let parsed = data;\n    if (Array.isArray(data)) {\n      console.warn(`YAML array is not supported in ${_id}, moving the array into the \\`body\\` key`);\n      parsed = { body: data };\n    }\n    return {\n      ...parsed,\n      _id,\n      _type: \"yaml\"\n    };\n  }\n});\n","import { destr } from \"destr\";\nimport { defineTransformer } from \"./utils.js\";\nexport default defineTransformer({\n  name: \"Json\",\n  extensions: [\".json\", \".json5\"],\n  parse: async (_id, content) => {\n    let parsed;\n    if (typeof content === \"string\") {\n      if (_id.endsWith(\"json5\")) {\n        parsed = (await import(\"json5\").then((m) => m.default || m)).parse(content);\n      } else if (_id.endsWith(\"json\")) {\n        parsed = destr(content);\n      }\n    } else {\n      parsed = content;\n    }\n    if (Array.isArray(parsed)) {\n      console.warn(`JSON array is not supported in ${_id}, moving the array into the \\`body\\` key`);\n      parsed = {\n        body: parsed\n      };\n    }\n    return {\n      ...parsed,\n      _id,\n      _type: \"json\"\n    };\n  }\n});\n","import { extname } from \"pathe\";\nimport { camelCase } from \"scule\";\nimport csv from \"./csv/index.js\";\nimport markdown from \"./markdown.js\";\nimport yaml from \"./yaml.js\";\nimport pathMeta from \"./path-meta.js\";\nimport json from \"./json.js\";\nconst TRANSFORMERS = [\n  csv,\n  markdown,\n  json,\n  yaml,\n  pathMeta\n];\nfunction getParser(ext, additionalTransformers = []) {\n  let parser = additionalTransformers.find((p) => ext.match(new RegExp(p.extensions.join(\"|\"), \"i\")) && p.parse);\n  if (!parser) {\n    parser = TRANSFORMERS.find((p) => ext.match(new RegExp(p.extensions.join(\"|\"), \"i\")) && p.parse);\n  }\n  return parser;\n}\nfunction getTransformers(ext, additionalTransformers = []) {\n  return [\n    ...additionalTransformers.filter((p) => ext.match(new RegExp(p.extensions.join(\"|\"), \"i\")) && p.transform),\n    ...TRANSFORMERS.filter((p) => ext.match(new RegExp(p.extensions.join(\"|\"), \"i\")) && p.transform)\n  ];\n}\nexport async function transformContent(id, content, options = {}) {\n  const { transformers = [] } = options;\n  const file = { _id: id, body: content };\n  const ext = extname(id);\n  const parser = getParser(ext, transformers);\n  if (!parser) {\n    console.warn(`${ext} files are not supported, \"${id}\" falling back to raw content`);\n    return file;\n  }\n  const parserOptions = options[camelCase(parser.name)] || {};\n  const parsed = await parser.parse(file._id, file.body, parserOptions);\n  const matchedTransformers = getTransformers(ext, transformers);\n  const result = await matchedTransformers.reduce(async (prev, cur) => {\n    const next = await prev || parsed;\n    const transformOptions = options[camelCase(cur.name)];\n    if (transformOptions === false) {\n      return next;\n    }\n    return cur.transform(next, transformOptions || {});\n  }, Promise.resolve(parsed));\n  return result;\n}\nexport { defineTransformer } from \"./utils.js\";\n","export function makeIgnored(ignores) {\n  const rxAll = [\"/\\\\.\", \"/-\", ...ignores.filter((p) => p)].map((p) => new RegExp(p));\n  return function isIgnored(key) {\n    const path = \"/\" + key.replace(/:/g, \"/\");\n    return rxAll.some((rx) => rx.test(path));\n  };\n}\n","import { assertArray, ensureArray, get } from \"./utils.js\";\nexport function createMatch(opts = {}) {\n  const operators = createOperators(match, opts.operators);\n  function match(item, conditions) {\n    if (typeof conditions !== \"object\" || conditions instanceof RegExp) {\n      return operators.$eq(item, conditions);\n    }\n    return Object.keys(conditions || {}).every((key) => {\n      const condition = conditions[key];\n      if (key.startsWith(\"$\") && operators[key]) {\n        const fn = operators[key];\n        return typeof fn === \"function\" ? fn(item, condition) : false;\n      }\n      return match(get(item, key), condition);\n    });\n  }\n  return match;\n}\nfunction createOperators(match, operators = {}) {\n  return {\n    $match: (item, condition) => match(item, condition),\n    /**\n     * Match if item equals condition\n     **/\n    $eq: (item, condition) => condition instanceof RegExp ? condition.test(item) : item === condition,\n    /**\n     * Match if item not equals condition\n     **/\n    $ne: (item, condition) => condition instanceof RegExp ? !condition.test(item) : item !== condition,\n    /**\n     * Match is condition is false\n     **/\n    $not: (item, condition) => !match(item, condition),\n    /**\n     * Match only if all of nested conditions are true\n     **/\n    $and: (item, condition) => {\n      assertArray(condition, \"$and requires an array as condition\");\n      return condition.every((cond) => match(item, cond));\n    },\n    /**\n     * Match if any of nested conditions is true\n     **/\n    $or: (item, condition) => {\n      assertArray(condition, \"$or requires an array as condition\");\n      return condition.some((cond) => match(item, cond));\n    },\n    /**\n     * Match if item is in condition array\n     **/\n    $in: (item, condition) => ensureArray(condition).some(\n      (cond) => Array.isArray(item) ? match(item, { $contains: cond }) : match(item, cond)\n    ),\n    /**\n     * Match if item contains every condition or match every rule in condition array\n     **/\n    $contains: (item, condition) => {\n      item = Array.isArray(item) ? item : String(item);\n      return ensureArray(condition).every((i) => item.includes(i));\n    },\n    /**\n     * Ignore case contains\n     **/\n    $icontains: (item, condition) => {\n      if (typeof condition !== \"string\") {\n        throw new TypeError(\"$icontains requires a string, use $contains instead\");\n      }\n      item = String(item).toLocaleLowerCase();\n      return ensureArray(condition).every((i) => item.includes(i.toLocaleLowerCase()));\n    },\n    /**\n     * Match if item contains at least one rule from condition array\n     */\n    $containsAny: (item, condition) => {\n      assertArray(condition, \"$containsAny requires an array as condition\");\n      item = Array.isArray(item) ? item : String(item);\n      return condition.some((i) => item.includes(i));\n    },\n    /**\n     * Check key existence\n     */\n    $exists: (item, condition) => condition ? typeof item !== \"undefined\" : typeof item === \"undefined\",\n    /**\n     * Match if type of item equals condition\n     */\n    $type: (item, condition) => typeof item === String(condition),\n    /**\n     * Provides regular expression capabilities for pattern matching strings.\n     */\n    $regex: (item, condition) => {\n      if (!(condition instanceof RegExp)) {\n        const matched = String(condition).match(/\\/(.*)\\/([dgimsuy]*)$/);\n        condition = matched?.[1] ? new RegExp(matched[1], matched[2] || \"\") : new RegExp(condition);\n      }\n      return condition.test(String(item || \"\"));\n    },\n    /**\n     * Check if item is less than condition\n     */\n    $lt: (item, condition) => {\n      return item < condition;\n    },\n    /**\n     * Check if item is less than or equal to condition\n     */\n    $lte: (item, condition) => {\n      return item <= condition;\n    },\n    /**\n     * Check if item is greater than condition\n     */\n    $gt: (item, condition) => {\n      return item > condition;\n    },\n    /**\n     * Check if item is greater than or equal to condition\n     */\n    $gte: (item, condition) => {\n      return item >= condition;\n    },\n    ...operators || {}\n  };\n}\n","import { joinURL } from \"ufo\";\nimport { apply, ensureArray, sortList, withoutKeys, withKeys, omit } from \"./utils.js\";\nimport { createMatch } from \"./index.js\";\nexport function createPipelineFetcher(getContentsList) {\n  const match = createMatch();\n  const surround = (data, { query, before, after }) => {\n    const matchQuery = typeof query === \"string\" ? { _path: query } : query;\n    const index = data.findIndex((item) => match(item, matchQuery));\n    before = before ?? 1;\n    after = after ?? 1;\n    const slice = new Array(before + after).fill(null, 0);\n    return index === -1 ? slice : slice.map((_, i) => data[index - before + i + Number(i >= before)] || null);\n  };\n  const matchingPipelines = [\n    // Conditions\n    (state, params) => {\n      const filtered = state.result.filter((item) => ensureArray(params.where).every((matchQuery) => match(item, matchQuery)));\n      return {\n        ...state,\n        result: filtered,\n        total: filtered.length\n      };\n    },\n    // Sort data\n    (state, params) => ensureArray(params.sort).forEach((options) => sortList(state.result, options)),\n    function fetchSurround(state, params, db) {\n      if (params.surround) {\n        let _surround = surround(state.result?.length === 1 ? db : state.result, params.surround);\n        _surround = apply(withoutKeys(params.without))(_surround);\n        _surround = apply(withKeys(params.only))(_surround);\n        state.surround = _surround;\n      }\n      return state;\n    }\n  ];\n  const transformingPiples = [\n    // Skip first items\n    (state, params) => {\n      if (params.skip) {\n        return {\n          ...state,\n          result: state.result.slice(params.skip),\n          skip: params.skip\n        };\n      }\n    },\n    // Pick first items\n    (state, params) => {\n      if (params.limit) {\n        return {\n          ...state,\n          result: state.result.slice(0, params.limit),\n          limit: params.limit\n        };\n      }\n    },\n    function fetchDirConfig(state, params, db) {\n      if (params.dirConfig) {\n        const path = state.result[0]?._path || params.where?.find((w) => w._path)?._path;\n        if (typeof path === \"string\") {\n          const dirConfig = db.find((item) => item._path === joinURL(path, \"_dir\"));\n          if (dirConfig) {\n            state.dirConfig = { _path: dirConfig._path, ...withoutKeys([\"_\"])(dirConfig) };\n          }\n        }\n      }\n      return state;\n    },\n    // Remove unwanted fields\n    (state, params) => ({\n      ...state,\n      result: apply(withoutKeys(params.without))(state.result)\n    }),\n    // Select only wanted fields\n    (state, params) => ({\n      ...state,\n      result: apply(withKeys(params.only))(state.result)\n    })\n  ];\n  return async (query) => {\n    const db = await getContentsList();\n    const params = query.params();\n    const result1 = {\n      result: db,\n      limit: 0,\n      skip: 0,\n      total: db.length\n    };\n    const matchedData = matchingPipelines.reduce(($data, pipe) => pipe($data, params, db) || $data, result1);\n    if (params.count) {\n      return {\n        result: matchedData.result.length\n      };\n    }\n    const result = transformingPiples.reduce(($data, pipe) => pipe($data, params, db) || $data, matchedData);\n    if (params.first) {\n      return {\n        ...omit([\"skip\", \"limit\", \"total\"])(result),\n        result: result.result[0]\n      };\n    }\n    return result;\n  };\n}\n","import { isPreview } from \"./preview.js\";\nimport { cacheStorage, chunksFromArray, getContent, getContentsList } from \"./storage.js\";\nimport { useRuntimeConfig } from \"#imports\";\nexport async function getContentIndex(event) {\n  const defaultLocale = useRuntimeConfig().content.defaultLocale;\n  let contentIndex = await cacheStorage().getItem(\"content-index.json\");\n  if (!contentIndex) {\n    const data = await getContentsList(event);\n    contentIndex = data.reduce((acc, item) => {\n      acc[item._path] = acc[item._path] || [];\n      if (item._locale === defaultLocale) {\n        acc[item._path].unshift(item._id);\n      } else {\n        acc[item._path].push(item._id);\n      }\n      return acc;\n    }, {});\n    await cacheStorage().setItem(\"content-index.json\", contentIndex);\n  }\n  return contentIndex;\n}\nexport async function getIndexedContentsList(event, query) {\n  const params = query.params();\n  const path = params?.where?.find((wh) => wh._path)?._path;\n  if (!isPreview(event) && !params.surround && !params.dirConfig && (typeof path === \"string\" || path instanceof RegExp)) {\n    const index = await getContentIndex(event);\n    const keys = Object.keys(index).filter((key) => path.test ? path.test(key) : key === String(path)).flatMap((key) => index[key]);\n    const keyChunks = [...chunksFromArray(keys, 10)];\n    const contents = [];\n    for await (const chunk of keyChunks) {\n      const result = await Promise.all(chunk.map((key) => getContent(event, key)));\n      contents.push(...result);\n    }\n    return contents;\n  }\n  return getContentsList(event);\n}\n","import { prefixStorage } from \"unstorage\";\nimport { joinURL, withLeadingSlash, withoutTrailingSlash } from \"ufo\";\nimport { hash as ohash } from \"ohash\";\nimport defu from \"defu\";\nimport { createQuery } from \"../query/query.js\";\nimport { transformContent } from \"../transformers/index.js\";\nimport { makeIgnored } from \"../utils/config.js\";\nimport { createPipelineFetcher } from \"../query/match/pipeline.js\";\nimport { getPreview, isPreview } from \"./preview.js\";\nimport { getIndexedContentsList } from \"./content-index.js\";\nimport { useNitroApp, useRuntimeConfig, useStorage } from \"#imports\";\nimport { transformers as customTransformers } from \"#content/virtual/transformers\";\nlet _sourceStorage;\nlet _cacheStorage;\nlet _cacheParsedStorage;\nexport const sourceStorage = () => {\n  if (!_sourceStorage) {\n    _sourceStorage = prefixStorage(useStorage(), \"content:source\");\n  }\n  return _sourceStorage;\n};\nexport const cacheStorage = () => {\n  if (!_cacheStorage) {\n    _cacheStorage = prefixStorage(useStorage(), \"cache:content\");\n  }\n  return _cacheStorage;\n};\nexport const cacheParsedStorage = () => {\n  if (!_cacheParsedStorage) {\n    _cacheParsedStorage = prefixStorage(useStorage(), \"cache:content:parsed\");\n  }\n  return _cacheParsedStorage;\n};\nconst isProduction = process.env.NODE_ENV === \"production\";\nconst isPrerendering = import.meta.prerender;\nconst contentConfig = () => useRuntimeConfig().content;\nconst invalidKeyCharacters = `'\"?#/`.split(\"\");\nconst contentIgnorePredicate = (key) => {\n  const isIgnored = makeIgnored(contentConfig().ignores);\n  if (key.startsWith(\"preview:\") || isIgnored(key)) {\n    return false;\n  }\n  if (invalidKeyCharacters.some((ik) => key.includes(ik))) {\n    console.warn(`Ignoring [${key}]. File name should not contain any of the following characters: ${invalidKeyCharacters.join(\", \")}`);\n    return false;\n  }\n  return true;\n};\nexport const getContentsIds = async (event, prefix) => {\n  let keys = [];\n  if (isProduction) {\n    keys = await cacheParsedStorage().getKeys(prefix);\n  }\n  const source = sourceStorage();\n  if (keys.length === 0) {\n    keys = await source.getKeys(prefix);\n  }\n  if (isPreview(event)) {\n    const { key } = getPreview(event);\n    const previewPrefix = `preview:${key}:${prefix || \"\"}`;\n    const previewKeys = await source.getKeys(previewPrefix);\n    if (previewKeys.length) {\n      const keysSet = new Set(keys);\n      await Promise.all(\n        previewKeys.map(async (key2) => {\n          const meta = await source.getMeta(key2);\n          if (meta?.__deleted) {\n            keysSet.delete(key2.substring(previewPrefix.length));\n          } else {\n            keysSet.add(key2.substring(previewPrefix.length));\n          }\n        })\n      );\n      keys = Array.from(keysSet);\n    }\n  }\n  return keys.filter(contentIgnorePredicate);\n};\nexport function* chunksFromArray(arr, n) {\n  for (let i = 0; i < arr.length; i += n) {\n    yield arr.slice(i, i + n);\n  }\n}\nlet cachedContents = [];\nexport const cleanCachedContents = () => {\n  cachedContents = [];\n};\nexport const getContentsList = /* @__PURE__ */ (() => {\n  let pendingContentsListPromise = null;\n  const _getContentsList = async (event, prefix) => {\n    const keys = await getContentsIds(event, prefix);\n    const keyChunks = [...chunksFromArray(keys, 10)];\n    const contents = [];\n    for (const chunk of keyChunks) {\n      const result = await Promise.all(chunk.map((key) => getContent(event, key)));\n      contents.push(...result);\n    }\n    return contents.filter((c) => c && c._path);\n  };\n  return (event, prefix) => {\n    if (event.context.__contentList) {\n      return event.context.__contentList;\n    }\n    if ((isPrerendering || !isProduction) && cachedContents.length) {\n      return cachedContents;\n    }\n    if (!pendingContentsListPromise) {\n      pendingContentsListPromise = _getContentsList(event, prefix);\n      pendingContentsListPromise.then((result) => {\n        if (isPrerendering || !isProduction) {\n          cachedContents = result;\n        }\n        event.context.__contentList = result;\n        pendingContentsListPromise = null;\n      });\n    }\n    return pendingContentsListPromise;\n  };\n})();\nconst pendingPromises = {};\nexport const getContent = async (event, id) => {\n  const contentId = id;\n  if (!contentIgnorePredicate(id)) {\n    return { _id: contentId, body: null };\n  }\n  const source = sourceStorage();\n  const cache = cacheParsedStorage();\n  if (isPreview(event)) {\n    const { key } = getPreview(event);\n    const previewId = `preview:${key}:${id}`;\n    const draft = await source.getItem(previewId);\n    if (draft) {\n      id = previewId;\n    }\n  }\n  const cached = await cache.getItem(id);\n  if (isProduction && cached) {\n    return cached.parsed;\n  }\n  const config = contentConfig();\n  const meta = await source.getMeta(id);\n  const mtime = meta.mtime;\n  const size = meta.size || 0;\n  const hash = ohash({\n    // Last modified time\n    mtime,\n    // File size\n    size,\n    // Add Content version to the hash, to revalidate the cache on content update\n    version: config.cacheVersion,\n    integrity: config.cacheIntegrity\n  });\n  if (cached?.hash === hash) {\n    return cached.parsed;\n  }\n  if (!pendingPromises[id + hash]) {\n    pendingPromises[id + hash] = new Promise(async (resolve) => {\n      const body = await source.getItem(id);\n      if (body === null) {\n        return resolve({ _id: contentId, body: null });\n      }\n      const parsed = await parseContent(contentId, body);\n      await cache.setItem(id, { parsed, hash }).catch(() => {\n      });\n      resolve(parsed);\n      delete pendingPromises[id + hash];\n    });\n  }\n  return pendingPromises[id + hash];\n};\nexport const parseContent = async (id, content, opts = {}) => {\n  const nitroApp = useNitroApp();\n  const config = contentConfig();\n  const options = defu(\n    opts,\n    {\n      markdown: {\n        ...config.markdown,\n        highlight: config.highlight\n      },\n      csv: config.csv,\n      yaml: config.yaml,\n      transformers: customTransformers,\n      pathMeta: {\n        defaultLocale: config.defaultLocale,\n        locales: config.locales,\n        respectPathCase: config.respectPathCase\n      }\n    }\n  );\n  const file = { _id: id, body: typeof content === \"string\" ? content.replace(/\\r\\n|\\r/g, \"\\n\") : content };\n  await nitroApp.hooks.callHook(\"content:file:beforeParse\", file);\n  const result = await transformContent(id, file.body, options);\n  await nitroApp.hooks.callHook(\"content:file:afterParse\", result);\n  return result;\n};\nexport const createServerQueryFetch = (event) => (query) => {\n  return createPipelineFetcher(() => getIndexedContentsList(event, query))(query);\n};\nexport function serverQueryContent(event, query, ...pathParts) {\n  const { advanceQuery } = useRuntimeConfig().public.content.experimental;\n  const config = contentConfig();\n  const queryBuilder = advanceQuery ? createQuery(createServerQueryFetch(event), { initialParams: typeof query !== \"string\" ? query || {} : {}, legacy: false }) : createQuery(createServerQueryFetch(event), { initialParams: typeof query !== \"string\" ? query || {} : {}, legacy: true });\n  let path;\n  if (typeof query === \"string\") {\n    path = withLeadingSlash(joinURL(query, ...pathParts));\n  }\n  const originalParamsFn = queryBuilder.params;\n  queryBuilder.params = () => {\n    const params = originalParamsFn();\n    if (path) {\n      params.where = params.where || [];\n      if (params.first && (params.where || []).length === 0) {\n        params.where.push({ _path: withoutTrailingSlash(path) });\n      } else {\n        params.where.push({ _path: new RegExp(`^${path.replace(/[-[\\]{}()*+.,^$\\s/]/g, \"\\\\$&\")}`) });\n      }\n    }\n    if (!params.sort?.length) {\n      params.sort = [{ _stem: 1, $numeric: true }];\n    }\n    if (!import.meta.dev) {\n      params.where = params.where || [];\n      if (!params.where.find((item) => typeof item._draft !== \"undefined\")) {\n        params.where.push({ _draft: { $ne: true } });\n      }\n    }\n    if (config.locales.length) {\n      const queryLocale = params.where?.find((w) => w._locale)?._locale;\n      if (!queryLocale) {\n        params.where = params.where || [];\n        params.where.push({ _locale: config.defaultLocale });\n      }\n    }\n    return params;\n  };\n  return queryBuilder;\n}\n"],"names":["hash","ohash","customTransformers"],"mappings":"","x_google_ignoreList":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}